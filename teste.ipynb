{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bb52221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning taco_4_edicao_ampliada_e_revisada.pdf from page 29 to 68...\n",
      "Formatting data...\n",
      "Extraction Complete! 597 items saved to taco_final.csv\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def is_nutrient_value(val):\n",
    "    \"\"\"\n",
    "    Checks if a string looks like a nutrient value.\n",
    "    Valid formats: '123', '12,3', 'Tr', 'NA', '*', empty string.\n",
    "    Invalid: 'Arroz', 'Cozido', long text.\n",
    "    \"\"\"\n",
    "    if not val: return True # Empty cells are common in data\n",
    "    val = str(val).strip().replace('\\n', ' ')\n",
    "    if val in ['NA', 'Tr', '*', '']: return True\n",
    "    # Check for numbers (allow comma or dot)\n",
    "    if re.match(r'^[\\d,.]+$', val): return True\n",
    "    return False\n",
    "\n",
    "def extract_taco_robust(pdf_path, output_csv):\n",
    "    data_store = {} # Key: ID, Value: {name, macros: [], micros: []}\n",
    "    \n",
    "    # Page Ranges (Based on TACO 4th Ed)\n",
    "    # Tables typically start on pg 29 (index 28) and end around pg 67 (index 66)\n",
    "    start_page = 28\n",
    "    end_page = 68\n",
    "\n",
    "    print(f\"Scanning {pdf_path} from page {start_page + 1} to {end_page}...\")\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i in range(start_page, end_page):\n",
    "            page = pdf.pages[i]\n",
    "            text = page.extract_text() or \"\"\n",
    "            \n",
    "            # --- Identify Page Type ---\n",
    "            page_type = None\n",
    "            if \"Umidade\" in text and \"Proteína\" in text:\n",
    "                page_type = \"MACRO\"\n",
    "            elif \"Manganês\" in text and \"Fósforo\" in text:\n",
    "                page_type = \"MICRO\"\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # Extract table using \"text\" strategy (best for grabbing all words)\n",
    "            # We assume stream logic because lines are sometimes missing in PDF metadata\n",
    "            table = page.extract_table(table_settings={\n",
    "                \"vertical_strategy\": \"text\", \n",
    "                \"horizontal_strategy\": \"text\",\n",
    "                \"intersection_tolerance\": 10\n",
    "            })\n",
    "\n",
    "            if not table: continue\n",
    "\n",
    "            for row in table:\n",
    "                # Clean basic whitespace\n",
    "                row = [str(cell).replace('\\n', ' ').strip() if cell else \"\" for cell in row]\n",
    "                \n",
    "                # Filter noise rows\n",
    "                if not row or len(row) < 5: continue\n",
    "                \n",
    "                # Find ID (First numeric looking item in the first 2 slots)\n",
    "                food_id = None\n",
    "                id_idx = -1\n",
    "                \n",
    "                if row[0].isdigit():\n",
    "                    food_id = row[0]\n",
    "                    id_idx = 0\n",
    "                elif len(row) > 1 and row[1].isdigit():\n",
    "                    food_id = row[1]\n",
    "                    id_idx = 1\n",
    "                \n",
    "                if not food_id: continue # Header or category row\n",
    "\n",
    "                # Ensure ID exists in store\n",
    "                if food_id not in data_store:\n",
    "                    data_store[food_id] = {'id': food_id, 'name': '', 'macros': [], 'micros': []}\n",
    "\n",
    "                # --- RIGHT-TO-LEFT PARSING ---\n",
    "                if page_type == \"MACRO\":\n",
    "                    # Expected standard columns after name: \n",
    "                    # [Umidade, kcal, kJ, Prot, Lip, Chol, Carb, Fiber, Ash, Ca, Mg] = 11 cols\n",
    "                    # Sometimes kcal/kJ are merged -> 10 cols\n",
    "                    \n",
    "                    data_chunk = []\n",
    "                    # Try taking last 11\n",
    "                    if len(row) >= 11 + (id_idx + 1):\n",
    "                        potential_data = row[-11:]\n",
    "                        # Check validity of this chunk\n",
    "                        if sum(is_nutrient_value(x) for x in potential_data) >= 8: # Tolerance for some weird OCR\n",
    "                            data_chunk = potential_data\n",
    "                            name_chunk = row[id_idx+1 : -11]\n",
    "                        else:\n",
    "                            # Try 10 columns (Energy merged)\n",
    "                            potential_data = row[-10:]\n",
    "                            if sum(is_nutrient_value(x) for x in potential_data) >= 7:\n",
    "                                # Split the merged Energy column (Col 1 in this chunk)\n",
    "                                # value \"517 124\" -> \"517\", \"124\"\n",
    "                                energy = potential_data[1].split(' ')\n",
    "                                if len(energy) == 2:\n",
    "                                    data_chunk = [potential_data[0]] + energy + potential_data[2:]\n",
    "                                else:\n",
    "                                    # Fallback if split fails\n",
    "                                    data_chunk = [potential_data[0], potential_data[1], \"\"] + potential_data[2:]\n",
    "                                name_chunk = row[id_idx+1 : -10]\n",
    "                    \n",
    "                    if data_chunk:\n",
    "                        # Reconstruct Name\n",
    "                        full_name = \" \".join(name_chunk).strip()\n",
    "                        # Fix common PDF join issues\n",
    "                        full_name = full_name.replace(\" ,\", \",\").replace(\"  \", \" \")\n",
    "                        \n",
    "                        if len(full_name) > 2:\n",
    "                            data_store[food_id]['name'] = full_name\n",
    "                        data_store[food_id]['macros'] = data_chunk\n",
    "\n",
    "                elif page_type == \"MICRO\":\n",
    "                    # Expected columns after ID:\n",
    "                    # [Mn, P, Fe, Na, K, Cu, Zn, Retinol, RE, RAE, B1, B2, B6, Niacin, VitC] = 15 cols\n",
    "                    target_cols = 15\n",
    "                    \n",
    "                    if len(row) >= target_cols + 1: # +1 for ID\n",
    "                        # Take last 15\n",
    "                        data_chunk = row[-target_cols:]\n",
    "                        # Simple validation\n",
    "                        if sum(is_nutrient_value(x) for x in data_chunk) >= 10:\n",
    "                            data_store[food_id]['micros'] = data_chunk\n",
    "\n",
    "    # --- Formatting for Output ---\n",
    "    print(\"Formatting data...\")\n",
    "    final_rows = []\n",
    "    \n",
    "    headers_macro = [\"Umidade(%)\", \"Energia(kcal)\", \"Energia(kJ)\", \"Proteína(g)\", \"Lipídeos(g)\", \n",
    "                     \"Colesterol(mg)\", \"Carboidrato(g)\", \"Fibra(g)\", \"Cinzas(g)\", \"Cálcio(mg)\", \"Magnésio(mg)\"]\n",
    "    \n",
    "    headers_micro = [\"Manganês(mg)\", \"Fósforo(mg)\", \"Ferro(mg)\", \"Sódio(mg)\", \"Potássio(mg)\", \n",
    "                     \"Cobre(mg)\", \"Zinco(mg)\", \"Retinol(mcg)\", \"RE(mcg)\", \"RAE(mcg)\", \n",
    "                     \"Tiamina(mg)\", \"Riboflavina(mg)\", \"Piridoxina(mg)\", \"Niacina(mg)\", \"VitaminaC(mg)\"]\n",
    "\n",
    "    all_headers = [\"ID\", \"Alimento\"] + headers_macro + headers_micro\n",
    "\n",
    "    # Sort keys numerically\n",
    "    sorted_ids = sorted(data_store.keys(), key=lambda x: int(x) if x.isdigit() else 9999)\n",
    "\n",
    "    for fid in sorted_ids:\n",
    "        item = data_store[fid]\n",
    "        \n",
    "        # Pad macros if missing (rare, but safety check)\n",
    "        m_list = item['macros']\n",
    "        if len(m_list) < 11: m_list += [\"\"] * (11 - len(m_list))\n",
    "        if len(m_list) > 11: m_list = m_list[:11] # Trim if extraction was weird\n",
    "        \n",
    "        # Pad micros\n",
    "        mi_list = item['micros']\n",
    "        if len(mi_list) < 15: mi_list += [\"\"] * (15 - len(mi_list))\n",
    "        \n",
    "        full_row = [item['id'], item['name']] + m_list + mi_list\n",
    "        final_rows.append(full_row)\n",
    "\n",
    "    # Convert to DF\n",
    "    df = pd.DataFrame(final_rows, columns=all_headers)\n",
    "    \n",
    "    # Clean numeric format (Brazil , to Int'l .)\n",
    "    for col in all_headers[2:]:\n",
    "        df[col] = df[col].astype(str).str.replace(',', '.', regex=False)\n",
    "        # Remove any internal spaces in numbers e.g. \"1 000\" -> \"1000\"\n",
    "        df[col] = df[col].str.replace(' ', '', regex=False)\n",
    "\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Extraction Complete! {len(df)} items saved to {output_csv}\")\n",
    "\n",
    "# --- Run ---\n",
    "extract_taco_robust(\"taco_4_edicao_ampliada_e_revisada.pdf\", \"taco_final.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
